{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d35f830",
   "metadata": {},
   "source": [
    "## 2. Construcción de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a35000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "import sklearn.metrics as metrics  ##matriz de confusion\n",
    "from  sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#usando las libreiras tradicionales\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "#import xgboost as xgb\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora debemos importar las librerias de AX para poder realizar los experimentos!\n",
    "from ax.service.ax_client import AxClient\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting # solo para graficar de otra manera!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a1517",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.load('Datos/X_train.npy',allow_pickle='TRUE')\n",
    "Y_train=np.load('Datos/Y_train.npy',allow_pickle='TRUE')\n",
    "\n",
    "X_test=np.load('Datos/X_test.npy',allow_pickle='TRUE')\n",
    "Y_test=np.load('Datos/Y_test.npy',allow_pickle='TRUE')\n",
    "\n",
    "Y_train=to_categorical(Y_train)\n",
    "Y_test=to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4230ac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función retorna un multi-layer-perceptron model en Keras.\n",
    "# Utilizando algunos parametros que son los que van a ir variando.\n",
    "\n",
    "def get_keras_model(num_hidden_layers, \n",
    "                    num_neurons_per_layer, \n",
    "                    dropout_rate, \n",
    "                    activation):\n",
    "    \n",
    "    # Creamos el MLP \n",
    "    \n",
    "    # Definimos las capas\n",
    "    ## Capa de entrada con dropout\n",
    "    ## Esta es la forma más general de construir la red secuencial con KERAS.\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(X_train.shape[1],))  # input layer.\n",
    "    x = layers.Dropout(dropout_rate)(inputs) # dropout on the weights.\n",
    "    \n",
    "    # Agregamos capas ocultas con dropout.\n",
    "    for i in range(num_hidden_layers):\n",
    "        x = layers.Dense(num_neurons_per_layer, \n",
    "                         activation=activation)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Creamos la capa de salida\n",
    "    # Tiene dos neuronas de salida porque mis datos target, son dos columnas.\n",
    "    # Como es un problema de clasificación utilizo softmax que llevara los pesos a una probabilidad.\n",
    "    # Lo que al predecir podremos llevar facilmente a una sola columna y luego calcular matriz de confusion.\n",
    "    # Ojo: por si acaso, en la salida no hay dropout logicamente.\n",
    "    \n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    ##Finalmente, en esta linea construyo el modeo que esta secuencialmente conectado y tiene dropout\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "    \n",
    "# Esta funcion consume los hyperparametros and retorna un score (Cross validation).\n",
    "def keras_mlp_cv_score(parameterization, weight=None):\n",
    "    \n",
    "    ## Aqui creo la red que probaré usando la función de arriba.\n",
    "    model = get_keras_model(parameterization.get('num_hidden_layers'),\n",
    "                            parameterization.get('neurons_per_layer'),\n",
    "                            parameterization.get('dropout_rate'),\n",
    "                            parameterization.get('activation'))\n",
    "    \n",
    "    ##imprimimos el modelo, para ver que esta funcionando y ver la cantidad de parametros que estamos utilizando.\n",
    "    model.summary()\n",
    "    \n",
    "    ## Parametro en duro para poder configurar todas las redes con un largo fijo de EPOCAS\n",
    "    ## Más elegante sería dejarlo como hiperparametro pero fijo en un default.\n",
    "    NUM_EPOCHS = 50\n",
    "    \n",
    "    ############ \n",
    "    ## Acá configuramos el optimizador (adam, sgd, etc) y su learning rate.\n",
    "    opt = parameterization.get('optimizer')\n",
    "    opt = opt.lower()\n",
    "    \n",
    "    learning_rate = parameterization.get('learning_rate')\n",
    "    \n",
    "    if opt == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif opt == 'rms':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    \n",
    "    ###### LINEA CLAVE - SETEANDO LA COMPILACION\n",
    "    # Ahora configuramos el entrenamiento con los parametros anteriores y que van a ir variando!\n",
    "    # Estoy usando el error medio (no tiene mucho sentido en clasificación, pero para que pueda ver como se hace)\n",
    "    # Más adelante corregiremos para una red orientada a clasificar.\n",
    "    # En este caso lo que se quiere es MINIMIZAR el MSE! Si fuera accuracy, querriamos MAXIMIZAR\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=tf.keras.losses.MeanSquaredError(),\n",
    "                  metrics=['mse','accuracy'])\n",
    "    \n",
    "    ##Funcion utiliza la data como variable global.\n",
    "    data = X_train\n",
    "    #labels = y_train.values\n",
    "    labels = Y_train\n",
    "    \n",
    "    # se entrena el modelo usando el 20% como validation set.\n",
    "    res = model.fit(data, labels, epochs=NUM_EPOCHS, batch_size=parameterization.get('batch_size'),\n",
    "                    validation_split=0.2)\n",
    "    \n",
    "    # Finalmente, utilizamos las últimas de 10 epocas. Calculamos la media y desviación estandar del validation score.\n",
    "    last10_scores = np.array(res.history['val_accuracy'][-10:])\n",
    "    mean = last10_scores.mean()\n",
    "    sem = last10_scores.std()\n",
    "    \n",
    "    # Si el modelo no converge seteamos una loss alta!\n",
    "#    if np.isnan(mean):\n",
    "#        return 9999.0, 0.0\n",
    "\n",
    "    if np.isnan(mean):\n",
    "        return 0.0, 0.0\n",
    "\n",
    "\n",
    "    return mean, sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d656d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora la magia de AX\n",
    "# Aqui configuramos los hiperparametros que deseamos probar en nuestros experimentos\n",
    "# de manera bastante intuitiva.\n",
    "# Importantes:\n",
    "# \"type\" - puede ser un rango o puede ser choice.\n",
    "# Si es \"range:\" - entonces, debo definir los limites del rango con \"bounds:\" [a..b] entre a y b\n",
    "# y además debemos configurar de que manera se escogera el valor.. por ej. \"log_scale:\" es una escala logaritmica entre a y b\n",
    "# o puede ser \"value_type:\" de tipo \"int\" o sea que es un rango de valores enteros entre a y b\n",
    "#\n",
    "# Si es \"choice:\" entonces, son valores específicos que quiero probar.\n",
    "# Por lo tanto este parametro va de la mano con \"values:\" y ahi se le da la lista de valores.. [8, 16, 32, 64, 128, 256]\n",
    "# o puede ser \"values\": ['tanh', 'sigmoid', 'relu'] ó \"values\": ['adam', 'rms', 'sgd']\n",
    "#\n",
    "# Como comentamos hay otras formas de hacer esto, sin embargo, requieren mucho más texto y es mucho menos claro e intuitivo\n",
    "# que realizar las pruebas con AX.\n",
    "\n",
    "parameters=[\n",
    "    {\n",
    "        \"name\": \"learning_rate\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [0.0001, 0.5],\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"dropout_rate\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [0.01, 0.5],\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"num_hidden_layers\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1, 3],\n",
    "        \"value_type\": \"int\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"neurons_per_layer\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [4, 16],\n",
    "        \"value_type\": \"int\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"batch_size\",\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": [8, 16, 32, 64, 128, 256],\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"name\": \"activation\",\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": ['tanh', 'sigmoid', 'relu'],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"optimizer\",\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": ['adam', 'rms', 'sgd'],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b74576",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_plotting()\n",
    "\n",
    "ax_client = AxClient() #inicializo AX para los experimentos\n",
    "\n",
    "# creo el experimento, donde en la variable parameters ingresan mis hiperparametros.\n",
    "# luego en objective_name, configuro la métrica objetivo que deseo optimizar.\n",
    "# finalmente debo decirle si quiero minimizar ó maximizar (True/False)\n",
    "# MUY importante, en este caso, 'keras_cv' lo que indica es que optimizara la metrica que nosotros definimos\n",
    "# en el compile de nuestra red! En este caso, es el MSE el error medio.\n",
    "# Un buen MSE esta cerca de 0.. por eso, queremos minimizar \n",
    "ax_client.create_experiment(\n",
    "    name=\"keras_experiment\",\n",
    "    parameters=parameters,\n",
    "    objective_name='keras_cv',\n",
    "    minimize=False)\n",
    "\n",
    "def evaluate(parameters):\n",
    "    return {\"keras_cv\": keras_mlp_cv_score(parameters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fe581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aqui generamos 25 experimentos!! :-)\n",
    "for i in range(4):\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=evaluate(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all the trials.\n",
    "ax_client.get_trials_data_frame().sort_values('trial_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5cbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values = ax_client.get_best_parameters()\n",
    "\n",
    "# the best set of parameters.\n",
    "for k in best_parameters.items():\n",
    "  print(k)\n",
    "\n",
    "print()\n",
    "\n",
    "# the best score achieved.\n",
    "means, covariances = values\n",
    "print(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ec980",
   "metadata": {},
   "source": [
    "# Evaluando los mejores parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d917d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the full training set and test.\n",
    "keras_model = get_keras_model(best_parameters['num_hidden_layers'], \n",
    "                              best_parameters['neurons_per_layer'], \n",
    "                              best_parameters['dropout_rate'],\n",
    "                              best_parameters['activation'])\n",
    "\n",
    "opt = best_parameters['optimizer']\n",
    "opt = opt.lower()\n",
    "\n",
    "learning_rate = best_parameters['learning_rate']\n",
    "\n",
    "if opt == 'adam':\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "elif opt == 'rms':\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Specify the training configuration.\n",
    "keras_model.compile(optimizer=optimizer,\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mse','accuracy'])\n",
    "\n",
    "data = X_train\n",
    "#labels = y_train.values\n",
    "labels = Y_train\n",
    "\n",
    "hist = keras_model.fit(data, labels, epochs=NUM_EPOCHS, batch_size=best_parameters['batch_size'], validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98823580",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778bfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
